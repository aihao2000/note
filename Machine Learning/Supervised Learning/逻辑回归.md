# 逻辑回归(logistic regression)

## 模型

逻辑回归通常用于分类问题中，如下图，拟合出一条直线做预测并不合适。分类往往期望于有范围的模型输出，如0代表判别为假，1代表判别为真。

![image-20230129194540001](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129194540001.png)

因此逻辑回归的模型是s型的函数，往往用sigmoid function（logistic function）作为模型。

可以令$z=\vec w \cdot \vec x +b$，那么当z大于0时可预测为1,，小于0时，可预测为false。

$z=\vec w \cdot \vec x +b=0$即为分割线

![image-20230129203242621](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129203242621.png)

模型为
$$
f(\vec x)=\frac{1}{1+e^{-(\vec w \cdot \vec x+b)}}
$$

```python
def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """

    g = 1/(1+np.exp(-z))
   
    return g
```



## 损失函数

使用平方损失函数，参数关于总损失函数图像如下

![image-20230129215419034](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129215419034.png)

可以看出有非常不光滑，不是适合梯度下降，因此需要设计更是和线性回归得损失函数。

单样本损失函数（loss function）如下:
$$
L(f(\vec x^{(i)}),y^{(i)})=
\begin{cases}
-log(f(\vec x^{(i)}) \ \ \ \ \ \ \ \ \ \ \ \   y^{(i)}=1 \\
-log(1-f(\vec x^{(i)})) \ \ \ \ y^{(i)}=0
\end{cases}
$$
或者可以写成:
$$
L(f(x^{(i)},y^{(i)}))=y^{(i)}(-log(f(\vec x^{(i)}) )+(1-y^{(i)})(-log(1-f(\vec x^{(i)}))
$$


总损失函数(cost function)即
$$
J(w,b)=\frac{1}{m}\sum_{i=1}^m L(f(\vec x^{(i)}),y^{(i)})
$$


![image-20230129213622499](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129213622499.png)

![image-20230129213600500](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129213600500.png)



![image-20230129215604107](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129215604107.png)

## 梯度下降

![image-20230129225428296](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129225428296.png) 



$$
f(\vec x)=\frac{1}{1+e^{-(\vec w \cdot \vec x+b)}} \hfill \\
L(f(x^{(i)},y^{(i)}))=y^{(i)}(-log(f(\vec x^{(i)}) )+(1-y^{(i)})(-log(1-f(\vec x^{(i)})) \hfill \\
J(\vec w,b)=\frac{1}{m}\sum_{i=1}^m L(f(\vec x^{(i)}),y^{(i)}) \hfill 
$$

![175745820](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/175745820.jpg)

```python
def compute_gradient_logistic(X, y, w, b): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
    Returns
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))                           #(n,)
    dj_db = 0.

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]                       #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   #(n,)
    dj_db = dj_db/m                                   #scalar
        
    return dj_db, dj_dw  
```



![image-20230129235102410](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230129235102410.png)



## 避免过拟合

![image-20230130014149024](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230130014149024.png)

![image-20230130014206798](./%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.assets/image-20230130014206798.png)



```python
def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m  = X.shape[0]
    n  = len(w)
    cost = 0.
    for i in range(m):
        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot
        cost = cost + (f_wb_i - y[i])**2                               #scalar             
    cost = cost / (2 * m)                                              #scalar  
 
    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar

def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m,n  = X.shape
    cost = 0.
    for i in range(m):
        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot
        f_wb_i = sigmoid(z_i)                                          #scalar
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar
             
    cost = cost/m                                                      #scalar

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar

def compute_gradient_linear_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]                 
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]               
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m   
    
    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw
```

None
